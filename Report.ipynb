{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train an agent to navigate (and collect bananas!) in a large, square world. \n",
    "\n",
    "This report provides description of the implementation algorithms.\n",
    "\n",
    "More detailed description of the project and the corresponding files and procedures is provided in the [README](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning Algorithm\n",
    "\n",
    "Description of the learning algorithm, along with the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architectures for the neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture is simple fully connected neural network with two hidden layers, each with 64 units. The output of all layers except the last one pass through `ReLU` non-linearity.\n",
    "\n",
    "The model is saved in the file **model.py**. It is and used by the agent. In **agent.py** we have `from model import QNetwork`. Each  agent has two such networks, as will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm is implemented in the function `dqn()`.\n",
    "\n",
    "For each step, the agent performs an action\n",
    "\n",
    "    action = agent.act(state, eps)\n",
    "    \n",
    "then gets information from the environment about the next state, the reward and the game status:\n",
    "\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    \n",
    "Next the agent and environment states are updated:\n",
    "\n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "\n",
    "The `agent.step` step includes:\n",
    "\n",
    "- saving into memory the experienced state, action, reward, next state\n",
    "\n",
    "      self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "- sampling from the memory for training purposes\n",
    "\n",
    "      experiences = self.memory.sample()\n",
    "      \n",
    "- and learning\n",
    "\n",
    "      self.learn(experiences, GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent learning is implemented in the `learn()` module of the `class Agent()`. The algorithm is Deep Q-Networks (DQN) as described in the DQN paper (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "\n",
    "The loss function is defined as the Mean Squared Error (MSE) between **the expected Q-value**, based on the trained local network `self.qnetwork_local` and **the target Q-value**, based on the Bellman equation and a separete network `self.qnetwork_target`. The target network has the same architecture as the local network but its' parameters is updated slower by low-pass filtering the local network parameters ($\\theta_{target} = \\tau \\cdot \\theta_{local} + (1 - \\tau) \\cdot \\theta_{target}$) as implemented in the `soft_update()` method.\n",
    "\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "We have not played with the dafault training hyperparameters:\n",
    "\n",
    "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    BATCH_SIZE = 64         # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR = 5e-4               # learning rate \n",
    "    UPDATE_EVERY = 4        # how often to update the network\n",
    "    \n",
    "The neural network parameters were not optimized also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot of Rewards\n",
    "\n",
    "A plot of rewards per episode. We can see thatafter 500 episodes there is no further improvement and the average reward (over 100 episodes) is about +15. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training Scores](scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of episodes needed to solve the environment\n",
    "    \n",
    "We get a target average score of 14 for 460 episodes:\n",
    "\n",
    "    Environment solved in 460 episodes!\tAverage Score: 14.00\n",
    "    \n",
    "![Training Scores](scores2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ideas for Future Work\n",
    "\n",
    "- explore networks with more layers and neurons per layer for better performance;\n",
    "\n",
    "- explore different training parameters, e.g. `BATCH_SIZE` for faster training;\n",
    "\n",
    "- implement prioritized experience replay, Double DQN, or Dueling DQN (as recommended in the Deep Q-Networks lesson;\n",
    "\n",
    "- complete the optional challenge - learn directly from pixels as opposed from the current ray-based perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Mnih V., Kavukcuoglu K., Silver D., et.all., Human-level control through deep reinforcement learning, Nature, 2015\n",
    "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "\n",
    "Finding moving average from data points in Python, https://stackoverflow.com/a/34387987\n",
    "\n",
    "Udacity lesson on Deep Q-Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
